# Case Studies {#sec-cases}

Two case studies illustrate the application of DICE. These studies are designed to replicate and expand classic context effects within a more ecologically valid social media feed environment using DICE. The focus of the case studies is to illustrate the usage and resulting data streams of DICE rather than advancing theory. This section aims to achieve two objectives. First, the studies demonstrate the ability to manipulate entire feed compositions and sequences, rather than just individual social media posts. Second, they demonstrate how post-level dwell time measurements can be used to approximate attention, complementing insights gained from traditional self-report measures. Substantively, Case Study 1 examines how content surrounding a sponsored post affects brand perceptions, while Case Study 2 explores how the specific position of a post within a feed influences attention and brand recall.

```{r install_packages}
#| warning: false
#| output: false

options(repos = c(CRAN = "https://cran.r-project.org")) 


if (!requireNamespace("groundhog", quietly = TRUE)) {
    install.packages("groundhog")
}

pkgs <- c("magrittr", "data.table", "knitr", "stringr", "english", "moments", "devtools",
          "ggplot2", "patchwork", "scales",  "ggdist", "gghalves", "sjPlot", "gtsummary", "wesanderson", "ggsci",
          "stargazer", "gt", "gtsummary", "flextable", "kableExtra", "MOTE", "dplyr",
          "multilevelmediation", "mediation", "lme4", "optimx")

groundhog::groundhog.library(pkg = pkgs,
                             date = "2024-10-01")

rm(pkgs)
```

```{r seed}
set.seed(42)
```

```{r layout}
layout <- theme(panel.background = element_rect(fill = "white"),
                legend.key = element_rect(fill = "white"),
                panel.grid.major.y = element_line(colour = "grey", 
                                                  linewidth = 0.25),
                axis.ticks.y = element_blank(),
                panel.grid.major.x = element_blank(),
                axis.line.x.bottom = element_line(colour = "#000000", 
                                                  linewidth = 0.5),
                axis.line.y.left = element_blank(),
                plot.title = element_text(size = rel(1))
)
```

```{r colors}
c_negative <- "#F0941F"
c_positive   <- "#196774"

# c_positive <- "#377E39"
# c_negative <- "#7D3756"
# 
# c_positive <- "#009E73"
# c_negative <- "#CC79A7"

scale_color_custom_d <- function() {
  scale_color_manual(values = c(c_orange, c_teal))
}

scale_fill_custom_d <- function() {
  scale_fill_manual(values = c(c_orange, c_teal))
}

scale_color_custom_2d <- function() {
  scale_color_manual(values = c(c_positive, c_negative))
}

scale_fill_custom_2d <- function() {
  scale_fill_manual(values = c(c_positive, c_negative))
}
```

```{r helpers}

# Create function to get effect size stats
get_effect_size <- function(data) {
  es <- effectsize::cohens_d(brand_attitude ~ condition, 
                            data = data, 
                            pooled_sd = FALSE)
  list(
    d = round(es$Cohens_d, 3),
    ci_low = round(es$CI_low, 3),
    ci_high = round(es$CI_high, 3),
    n = nrow(data)
  )
}
```


## Case Study 1: Feed Composition and Context Effects {#sec-brand-safety-case}

```{r read_brazil_data}
short <- fread(file = "../../oFeeds/studies/brand_safety/data/processed/brand-safety-short.csv", na.strings = "")
long  <- fread(file = "../../oFeeds/studies/brand_safety/data/processed/brand-safety-long.csv", na.strings = "")
stimuli_1  <- fread(file = "../templates/docs/data/case-study-2/brazil.csv", na.strings = "")
```

```{r rename_conditions}
short[, condition := as.factor(ifelse(test = condition == "inappropriate", yes = "unsafe", no = "safe"))]
long[, condition := as.factor(ifelse(test = condition == "inappropriate", yes = "unsafe", no = "safe"))]
```

```{r}
all_short <- copy(short)
short <- short[is.finite(log_dwell_pixel)] # [is_flood_aware == FALSE]
```

```{r str-to-int}
long[, displayed_sequence := as.integer(displayed_sequence)]
long[, seconds_in_viewport := as.numeric(seconds_in_viewport)]
long[, log_dwell_pixel := as.numeric(log_dwell_pixel)]
long[, log_dwell_time := as.numeric(log_dwell_time)]
```

```{r subset_data}
subset <- long[is.finite(log_dwell_pixel) & 
                     displayed_sequence < 19 & 
                     displayed_sequence > 2]
```

```{r time-spent-measure}
short[, log_time_spent := log(time_spent_on_page - seconds_in_viewport)]
```

Case Study 1 demonstrates DICE’s capability to study context effects with high experimental control and study realism. This study illustrates how researchers can systematically manipulate the broader context (i.e., the composition of the feed) in which users encounter a specific post (in this case, a sponsored post from a brand). Substantively, Case Study 1 examines the issue of brand safety in social media advertising. Brand safety refers to the idea that advertising should not appear in contexts that could harm a brand’s reputation [@FournierSrinivasan_2023]. This concern is particularly relevant for social media advertising, where platforms use automated systems to place ads in dynamic, user-generated content environments. These systems often lack the nuanced understanding needed to identify potentially problematic contexts that could harm a brand. While industry reports suggest that up to 75% of brands have experienced such unsafe brand exposures [@AhmadEtAl_2024; @GumGum_2017], examining these effects in the field risks apparent brand damage.

### Experimental Design

To test how brand (un)safe contexts affect brand perceptions, we created two social media feeds that were identical in structure but varied in their content surrounding a sponsored post (see @fig-design-1^[The figure shows how DICE enables the controlled manipulation of feed contexts. The identical sponsored post by KLM (highlighted) appears in the “brand-safe” feeds (left) surrounded by neutral Brazil content versus “brand-unsafe” feeds (right) where it appears alongside posts about the Brazil flooding disaster. The sponsored post always remains in the fifth position, while surrounding organic posts are fully randomized. An example feed for the brand-unsafe condition is accessible at https://tiny.cc/DICE1 .] for exemplary screenshots of the brand (un)safe feeds). The sponsored post in both conditions was an ad by the airline KLM promoting flights to Brazil. 

In the brand-safe condition, the sponsored post was surrounded by actual organic posts covering Brazil scraped from the web. In the brand-unsafe condition, however, the sponsored post was surrounded by another set of scraped organic posts about the severe flooding that occurred during the time of the study. Such a situation is precisely the type of contextual mismatch that automated systems can create and managers fear due to the adverse consequences for brands [@AhmadEtAl_2024; @GumGum_2017]. In both conditions, the sponsored post was always fixed in the fifth position, whereas the order of the organic posts varied randomly.

![Exemplary DICE Feeds from Case Study 1](figures-cs2-design.png){#fig-design-1}

### Procedure

```{r device_type_shared_2}
#| echo: false
N <- short[, .N]
share_desktop <- round(100 * short[device_type == "Desktop", .N] / N)
share_mobile  <- round(100 * short[device_type == "Mobile", .N] / N)
share_tablet  <- round(100 * short[device_type == "Tablet", .N] / N)
```

```{r c_alpha}
#| echo: false
short[, brand_att_1 := as.numeric(as.character(brand_att_1))]
short[, brand_att_2 := as.numeric(as.character(brand_att_2))]
short[, brand_att_3 := as.numeric(as.character(brand_att_3))]

alpha <- psych::alpha(x = short[, .(brand_att_1, brand_att_2, brand_att_3)])
```

We recruited 982 US-American participants on Prolific ($M_{age} = `r short[, round(mean(age, na.rm = TRUE))]`$ years; `r round(short[, mean(female, na.rm = TRUE)] * 100)`% female) to participate in the study. Participants browsed the simulated feed on their own devices (`r share_desktop`% desktop, `r share_mobile`% mobile, and `r share_tablet`% tablet). After scrolling through the feed, participants were redirected to a Qualtrics survey in which they first provided demographic information as a filler task. Next, participants reported their brand attitude toward KLM using three seven-point scales (1 = "Negative/Unfavorable/Dislike" and 7 = "Positive/Favorable/Like"; $\alpha$ = `r round(alpha$total$raw_alpha, digits = 2)`). Finally, we assessed participants’ awareness of the Brazil flooding. For this and all studies, all stimuli, materials, data, and analysis code are available on the Open Science Framework (OSF): <https://osf.io/2xs5c/?view_only=4bf95d2a2c8449218b5fa7cd288f626a>. 

### Stimuli

Building on the CSV file structure introduced in Case Study 1, we created a file containing two distinct sets of content: nineteen organic posts for each experimental condition, plus one sponsored post that needed to appear in both feeds. To ensure the sponsored post would appear in both conditions while maintaining DICE's CSV structure, we entered the sponsored post twice in the file - once for each condition. This resulted in a file with forty rows total: nineteen organic posts for each condition plus the sponsored post appearing twice.
Each post's content was specified in columns such as `<text>` and `<username>`. We used the `<condition>` parameter to distinguish between our "appropriate" (brand-safe) and "inappropriate" (brand-unsafe) feeds, assigning each row (i.e., each post) to its respective condition.
Similar to Case Study 1, we left the `<sequence>` column empty for organic posts to enable randomization, with one key exception: the KLM sponsored post was assigned a fixed `<sequence>` value of "5" to ensure consistent positioning across conditions. We marked this post as sponsored using the `<sponsored>` parameter and included a KLM landing page URL in the `<target>` column for participants who clicked on the ad.
The resulting CSV file was uploaded to an online repository to generate a URL for the DICE app.

@tbl-stimuli-case-1 shows an excerpt of the exact CSV files we used to create the stimuli for this study. You can download that file [here](https://raw.githubusercontent.com/Howquez/DICE/refs/heads/main/studies/brand_safety/stimuli/brazil.csv){target="_blank"}.

```{r}
#| label: tbl-stimuli-case-1
#| tbl-cap: CSV File Used in Study 1

text_columns <- names(stimuli_1)[sapply(stimuli_1, is.character)]

stimuli_1[, (text_columns) := lapply(.SD, function(x) {
  x <- str_replace_all(x, "\n(?=[^.,!?;:])", " ")
  x <- str_replace_all(x, "\n(?=[.,!?;:])", "")
  x <- str_replace_all(x, "@", "")
  str_replace_all(x, "<br>", " ")
}), .SDcols = text_columns]

kable(stimuli_1[c(5, 16:23)])
```


```{r test_balance}
# Omnibus test of joint orthogonality with randomization inference

# Full model with covariates
full_model <- lm(as.numeric(as.factor(condition)) ~ female + age, data = short)

# Null model without covariates
null_model <- lm(as.numeric(as.factor(condition)) ~ 1, data = short)

anova_result <- anova(null_model, full_model)

# Observed F-statistic from ANOVA
observed_f_stat <- anova_result$F[2]

# Set up randomization inference
n_simulations <- 1000
simulated_f_stats <- numeric(length = n_simulations)

for (i in 1:n_simulations) {
  # Shuffle the treatment labels
  short[, shuffled_condition := sample(condition)]
  
  # Refit the full and null models with shuffled treatment
  shuffled_full_model <- lm(as.numeric(as.factor(shuffled_condition)) ~ female + age, data = short)
  shuffled_null_model <- lm(as.numeric(as.factor(shuffled_condition)) ~ 1, data = short)
  
  # Perform ANOVA on shuffled data
  shuffled_anova_result <- anova(shuffled_null_model, shuffled_full_model)
  
  # Store the F-statistic
  simulated_f_stats[i] <- shuffled_anova_result$F[2]
}

# Calculate the p-value based on randomization inference
balance_p <- sprintf(fmt = "%.2f", mean(simulated_f_stats >= observed_f_stat))


```

### Participants and Randomization Checks

Participants were randomly assigned to view either the brand-safe feed (featuring general Brazil-related content) or the brand-unsafe feed (featuring flood coverage). A key advantage of DICE over observational and platform studies is its ability to implement true random assignment, allowing us to isolate the effect of context while canceling out other factors that might influence brand perception.
To validate DICE's randomization functionality, we examined the balance between treatment groups: as illustrated in @tbl-balance, the two treatment groups do not exhibit differences in observables. Following @KerwinRostomSterck_2024, we also found support for balanced conditions in an omnibus test of joint orthogonality with randomization inference ($p=$ `r balance_p`).


```{r}
#| warning: false
#| label: tbl-balance 
#| tbl-cap: Covariate Balance Across Conditions

balance_fem <- lm(formula = female ~ condition, data = short)
balance_age <- lm(formula = age ~ condition, data = short)

balance_table <- data.table(variables = c("Mean Age (Years)", "Female (Percent)"),
                            appropriate = c(short[condition == "appropriate", 
                                                  mean(age, na.rm = TRUE)],
                                            short[condition == "appropriate",
                                                  mean(female, na.rm = TRUE)]*100),
                            inappropriate = c(short[condition == "inappropriate", 
                                                  mean(age, na.rm = TRUE)],
                                            short[condition == "inappropriate", 
                                                  mean(female, na.rm = TRUE)]*100),
                            difference = c(summary(balance_age)$coefficients[2, 1],
                                           summary(balance_fem)$coefficients[2, 1]*100),
                            pairwise_p = c(summary(balance_age)$coefficients[2, 4],
                                           summary(balance_fem)$coefficients[2, 4]))

balance_table %>% kable(digits = 3,
                        col.names = c("Covariate",
                                      "Appropriate",
                                      "Inappropriate",
                                      "Difference",
                                      "p-value"))

```


### Data

Our dataset comprises `r short[, length(unique(participant_label))]` participants and `r format(x = subset[, .N], big.mark = ",")` observations at the participant $\times$ post level. Whereas Case Study 1 analyzed multiple sponsored posts across participants, here we focus on a single sponsored post (i.e., the KLM ad) viewed by all participants, which simplifies our analytical approach. Our final sample comprises `r format(x = short[is.finite(log_dwell_pixel), .N], big.mark = ",")` observations on the participant level, which is slightly less than the expected one observation per participant due to connectivity issues: no dwell time data were recorded for around `r sprintf(fmt = "%.2f", 100 * (short[!is.finite(log_dwell_pixel), .N] / short[, length(unique(participant_label))]))`% of the sponsored posts. 

This simplified "short" data structure has two convenient methodological implications. First, because we analyze one observation per participant rather than nested data, we can apply simpler methods such as ordinary least squares (OLS) regressions in our analyses. Second, because we only focus on one sponsored post, we do not need to divide our dwell time measure by the post's height as we did in Case Study 1. Both aspects increase the interpretability of our results.

@tbl-show-data-2 shows an excerpt of the processed data to illustrate its nested (i.e., "long") structure.

```{r}
#| label: tbl-show-data-2
#| tbl-cap: Processed Data Analyzed in Study 2

tmp <- short[,
            .(`Participant ID` = participant_label,
              `Position in Feed` = 5,
              Likes = liked,
              Replies = hasReply,
              `Seconds in Viewport` = seconds_in_viewport,
              `Dwell Time` = log_dwell_time,
              `Brand Attitudes` = brand_attitude,
              Age = age, 
              Female = female, 
              Desktop = as.logical(is_desktop),
              Recall = klm_uncued_recall)]

kable(tmp[35:45])
```

### Results and Discussion

```{r regression models}
lm_main <- lm(brand_attitude ~ condition, data = short)

lm_x1 <- lm(brand_attitude ~ condition * log_dwell_time, data = short)
lm_x2 <- lm(brand_attitude ~ condition * log_dwell_time + log_time_spent, data = short)
lm_x3 <- lm(brand_attitude ~ condition * log_time_spent, data = short)

lm_dwell <- lm(log_dwell_time ~ condition, data = short)
```


```{r reporting_main_effect}

model_summary <- summary(lm_main)
df_main <- model_summary$df[2]

beta <- model_summary$coefficients[2, 1]  # F-statistic value
se   <- model_summary$coefficients[2, 2]  # SE
t_value <- model_summary$coefficients[2, 3]  
f_value <- model_summary$fstatistic[1]  # F-statistic value
f_df1 <- model_summary$fstatistic[2]    # degrees of freedom for the model
f_df2 <- model_summary$fstatistic[3]    # degrees of freedom for the residuals
p_value <- pf(f_value, f_df1, f_df2, lower.tail = FALSE)  # p-value from F-statistic
cohensD <- apa(effectsize::cohens_d(brand_attitude ~ condition, data = short, pooled_sd = FALSE)$Cohens_d, decimals = 3, leading = FALSE)
```


```{r reporting_moderation}
model_summary <- summary(lm_x1)

coefficients <- coef(model_summary)
df_moderation <- model_summary$df[2]  # Residual degrees of freedom

condition_beta <- coefficients[2, 1]
condition_se <- coefficients[2, 2]
condition_t <- coefficients[2, 3]
condition_p <- coefficients[2, 4]

dwell_beta <- coefficients[3, 1]
dwell_se <- coefficients[3, 2]
dwell_t <- coefficients[3, 3]
dwell_p <- coefficients[3, 4]

interaction_beta <- coefficients[4, 1]
interaction_se <- coefficients[4, 2]
interaction_t <- coefficients[4, 3]
interaction_p <- coefficients[4, 4]

```

```{r reporting_dwell_differences}
model_summary <- summary(lm_dwell)

coefficients <- coef(model_summary)
dwell_condition_df <- model_summary$df[2]

dwell_condition_beta <- coefficients[2, 1]
dwell_condition_se <- coefficients[2, 2]
dwell_condition_t <- coefficients[2, 3]
dwell_condition_p <- coefficients[2, 4]
```

```{r reporting_robustness}
model_summary <- summary(lm_x2)

coefficients <- coef(model_summary)
df_robustness <- model_summary$df[2]

robustness_beta <- coefficients[5, 1]
robustness_se <- coefficients[5, 2]
robustness_t <- coefficients[5, 3]
robustness_p <- coefficients[5, 4]
```

Brand attitudes toward KLM were significantly less positive in the brand-unsafe feed condition ($M_u = `r short[condition == "unsafe", apa(mean(brand_attitude, na.rm = TRUE), decimals = 3, leading = FALSE)]`$, $SD_u = `r short[condition == "unsafe", apa(sd(brand_attitude, na.rm = TRUE), decimals = 3, leading = FALSE)]`$) compared to the brand-safe feed condition
($M_s = `r short[condition == "safe", apa(mean(brand_attitude, na.rm = TRUE), decimals = 3, leading = FALSE)]`$, 
$SD_s = `r short[condition == "safe", apa(sd(brand_attitude, na.rm = TRUE), decimals = 3, leading = FALSE)]`$,
$b = `r apa(beta, decimals = 3, leading = FALSE)`$, 
$SE = `r apa(se, decimals = 3, leading = FALSE)`$, 
$t(`r df_main`)  = `r apa(t_value, decimals = 3, leading = FALSE)`$, 
$p = `r apa(p_value, decimals = 3, leading = FALSE)`$, 
$d = `r cohensD`$).

To further explore the interplay between the KLM ad’s context and brand attitudes, we examined whether the dwell time of the ad  moderated the previously reported main effect  of context. An OLS regression revealed a statistically significant interaction between the context’s brand safety and dwell time
($b = `r apa(interaction_beta, decimals = 3, leading = FALSE)`$,
$SE = `r apa(interaction_se, decimals = 3, leading = FALSE)`$,
$t(`r df_moderation`) = `r apa(interaction_t, decimals = 3, leading = FALSE)`$,
$p = `r apa(interaction_p, decimals = 3, leading = FALSE)`$),
indicating that the lack of attention shapes how context affects brand attitudes (see @fig-moderation-effects). This suggests that the negative effect of an unsafe context on brand attitude only emerged when participants spent a sufficient amount of time viewing the sponsored post. In contrast, among those participants with minimal dwell time, there was little difference in brand attitudes between safe and unsafe contexts. The main effects of brand safety 
($b = `r apa(condition_beta, decimals = 3, leading = FALSE)`$,
$SE = `r apa(condition_se, decimals = 3, leading = FALSE)`$,
$t(`r df_moderation`) = `r apa(condition_t, decimals = 3, leading = FALSE)`$,
$p = `r apa(condition_p, decimals = 3, leading = FALSE)`$) and dwell time 
($b = `r apa(dwell_beta, decimals = 3, leading = FALSE)`$,
$SE = `r apa(dwell_se, decimals = 3, leading = FALSE)`$,
$t(`r df_moderation`) = `r apa(dwell_t, decimals = 3, leading = FALSE)`$,
$p = `r apa(dwell_p, decimals = 3, leading = FALSE)`$)
were not significant. 
The ad’s dwell time did not vary across brand safety conditions 
($b = `r apa(dwell_condition_beta, decimals = 3, leading = FALSE)`$,
$SE = `r apa(dwell_condition_se, decimals = 3, leading = FALSE)`$,
$t(`r dwell_condition_df`) = `r apa(robustness_t, decimals = 3, leading = FALSE)`$,
$p = `r apa(dwell_condition_p, decimals = 3, leading = FALSE)`$). 
Finally, this moderation is robust to alternative model specifications (see @sec-robustness-checks-1) where we repeated the same analysis while controlling for the dwell time allocated to all organic posts
($b = `r apa(robustness_beta, decimals = 3, leading = FALSE)`$,
$SE = `r apa(robustness_se, decimals = 3, leading = FALSE)`$,
$t(`r df_robustness`) = `r apa(robustness_t, decimals = 3, leading = FALSE)`$,
$p = `r apa(robustness_p, decimals = 3, leading = FALSE)`$). 


```{r johnson_neyman}

q_95 <- short[, quantile(x = log_dwell_time, probs = 0.95, na.rm = TRUE)]
q_05 <- short[, quantile(x = log_dwell_time, probs = 0.05, na.rm = TRUE)]

# Find Johnson Neyman through simulation

# sub <- short[is.finite(log_dwell_time)]
sub <- copy(short)
tmp <- data.table(log_dwell_time = seq(from = min(sub$log_dwell_time, 
                                                           na.rm = TRUE),
                                       to = max(sub$log_dwell_time, 
                                                         na.rm = TRUE),
                                       length.out = 100),
                  condition = rep(x = c("safe", "unsafe"),
                                  each = 100))

lm_jn <- lm(brand_attitude ~ log_dwell_time * condition, data = sub)

predictions <- predict(lm_jn, newdata = tmp, interval = "confidence")

tmp[, c("fit", "lwr", "upr") := as.data.table(predictions)]

safe <- tmp[condition == "safe"]
unsafe <- tmp[condition == "unsafe"]

non_overlap_point <- which(safe$lwr > unsafe$upr)

jn_point <- safe$log_dwell_time[non_overlap_point[1]-1]
jn_seconds <- round(exp(jn_point), digits = 2)
```


```{r}
#| label: fig-moderation-effects
#| fig-cap: "Moderation of the Effect of Context on Brand Attitudes by Dwell Time"
#| fig-cap-location: top

sub[condition == "unsafe", condition := "Unsafe"]
sub[condition == "safe", condition := "Safe"]

ggplot(data = sub,
       mapping = aes(x = log_dwell_time,
                     y = brand_attitude,
                     fill = condition,
                     col = condition)) +
  geom_rect(aes(xmin = -Inf, xmax = jn_point, ymin = 1, ymax = 7.2),
            fill = "#cccccc", col = NA, alpha = 0.025) + 
  geom_vline(xintercept = jn_point, lty = 2) +
  geom_smooth(method = "lm", formula = "y ~ x") +
  layout +
  labs(x = "log(Dwell Time)",
       y = "Brand Attitude",
       # title = "Moderating Effect of Dwell Time on Brand Attitude",
       caption = paste0("Johnson-Neyman-Interval indicated by grey area.
                        Its cutoff (indicated by the dashed vertical line) translates to exp(",
                        round(jn_point, digits = 2), ") = ",
       jn_seconds,
       " seconds.
       The white (grey) area shows for which participants the effect of context is significant
       (non-significant)."), #The x-axis ranges from the 5th to the 95th percentile of the distribution.
       color = "",
       fill = "") +
  scale_y_continuous(expand = expansion(mult = c(0, 0)), breaks = 1:7) +
  scale_x_continuous(expand = expansion(mult = c(0, 0))) +
  coord_cartesian(xlim = c(q_05, q_95),
                  ylim = c(1, 7.2)) +
  scale_fill_custom_2d() +
  scale_color_custom_2d() +
  layout +
  theme(legend.position = "bottom")
  
```

From a substantive perspective, the findings provide experimental support for brand safety concerns and reveal an intuitive nuance: contextual misplacements primarily harm brand perceptions when consumers pay sufficient attention to the content. When attention is minimal (indicated by minimal dwell times), the negative impact of unsafe contexts appears to be neutralized. From a methodological perspective, this study illustrates how researchers can manipulate entire feed compositions rather than just single social media posts. It also showcases how DICE’s dwell time data can be interpreted as a proxy of attention. A lack of dwell time for a post, however, implies a lack of attention to this content.

### Robustness Checks {#sec-robustness-checks-1}

To test the robustness of our findings, we conducted additional analyses controlling for participants' dwell time on organic posts that constituted our experimental manipulation. Since the brand safety manipulation involved varying the content of organic posts surrounding the sponsored ad (brand-safe vs. brand-unsafe), it was important to verify that our results were not simply driven by differential attention to these organic posts across conditions.

Models 1 and 2 in Table @tbl-robustness feature the main findings reported in the manuscript. Model 3 adds the dwell time on all organic posts as a covariate, while Model 4 replaces the dwell time on the sponsored post with the dwell time on the organic posts. Both models demonstrate that our key findings (i.e., the main effects as well as the interaction between brand safety and dwell time) were robust. The interaction effect (Brand-Unsafe × Dwell Time KLM) remained significant, indicating that the moderating role of attention on context effects persists even after accounting for engagement with the manipulated organic content. Model 4 additionally shows that dwell time on organic posts also moderates the brand safety effect.

```{r}
#| eval: true
#| warning: false
#| label: tbl-robustness
#| tbl-cap: Estimates of Brand Attitude as a Function of Brand Safety and Dwell Times
#| results: asis

n_obs <- format(nobs(lm_x1), big.mark = ",")

tbl_1 <- tbl_regression(
  lm_main, 
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE,
  show_single_row = "condition",
  intercept = TRUE,
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = r.squared)

tbl_2 <- tbl_regression(
  lm_x1, 
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE, 
  show_single_row = "condition",
  intercept = TRUE,
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = r.squared)

tbl_3 <- tbl_regression(
  lm_x2, 
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE,
  show_single_row = "condition",
  intercept = TRUE,
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = r.squared)

tbl_4 <- tbl_regression(
  lm_x3, 
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE, 
  show_single_row = "condition",
  intercept = TRUE,
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = r.squared)

table <- tbl_merge(
  tbls = list(tbl_1, tbl_2, tbl_3, tbl_4),
  tab_spanner = c("Model 1", "Model 2", "Model 3", "Model 4")
) |>
  # Remove empty rows
  modify_table_body(
    ~ .x |>
      dplyr::filter(
        !(is.na(estimate_1) & is.na(estimate_2) & is.na(estimate_3) & is.na(estimate_4))
      )
  ) |>
  modify_table_body(
    ~ .x |>
      dplyr::mutate(
        label = case_when(
          label == "(Intercept)" ~ "Constant",
          label == "condition" ~ "Brand-Unsafe",
          label == "log_dwell_time" ~ "Dwell Time (KLM)",
          label == "log_time_spent" ~ "Dwell Time (Organic)",
          label == "unsafe * log_dwell_time" ~ "Brand-Unsafe × Dwell Time (KLM)",
          label == "unsafe * log_time_spent" ~ "Brand-Unsafe × Dwell Time (Organic)",
          label == "condition * log_dwell_time" ~ "Brand-Unsafe × Dwell Time (KLM)",
          label == "condition * log_time_spent" ~ "Brand-Unsafe × Dwell Time (Organic)",
          TRUE ~ label
        )
      )
  ) |>
  modify_table_body(
    ~ .x |>
      dplyr::arrange(
        factor(label, levels = c(
          "Brand-Unsafe",
          "Dwell Time (KLM)",
          "Dwell Time (Organic)",
          "Brand-Unsafe × Dwell Time (KLM)",
          "Brand-Unsafe × Dwell Time (Organic)",
          "Constant",
          "R²"
        ))
      )
  ) |>
  modify_header(
    label ~ "",
    estimate_1 ~ "b",
    estimate_2 ~ "b",
    estimate_3 ~ "b",
    estimate_4 ~ "b",
    std.error_1 ~ "SE",
    std.error_2 ~ "SE",
    std.error_3 ~ "SE",
    std.error_4 ~ "SE",
    p.value_1 ~ "p",
    p.value_2 ~ "p",
    p.value_3 ~ "p",
    p.value_4 ~ "p"
  )

table
```

## Case Study 2: Position Effects and Competing Attention Between Brands {#sec-positioning-case}

```{r read_meme_data}
data <- fread(file = "../../oFeeds/studies/meme_feed/data/processed/meme-feed-data.csv", na.strings = "")
stimuli_2  <- fread(file = "../templates/docs/data/case-study-1/9gag.csv", na.strings = "")
```

```{r subset_meme_data}
subset <- data[is.finite(log_dwell_pixel) & 
                     displayed_sequence < 39 & 
                     displayed_sequence > 2]
```


Rather than focusing on a single post, Case Study 2 examines how multiple brands compete for user attention within the same feed—a common scenario in social media advertising where multiple advertisers target the same user and try to “cut through the clutter” of both organic content and competing ads [@Ordenes_2019]. Case Study 2 also illustrates how to approximate attention patterns across an entire feed containing multiple sponsored posts from different advertisers in the same product category. By studying how users engage with multiple sponsored posts in a single feed, we can better understand the dynamics of attention allocation in social media environments where brands compete for attention simultaneously, building on related memory and context effects studied in traditional advertising environments [@PietersWarlopWedel_2002]. Specifically, we examine how the position of sponsored posts affects brand recall when multiple brands are presented within the same feed, and how the dwell times captured through DICE help explain these effects.

### Experimental Design

To investigate the relationship between ad placement and recall, we simulated  [social media feeds](https://perma.cc/87B9-H6ZV){target="_blank"} containing both organic and sponsored posts. Whereas the set of organic and sponsored posts was the same for all participants, the sequence in which the participants were exposed to these posts was unique for every participant as we randomized the sequence between subjects.

To investigate the relationship between ad placement and brand recall, we simulated a social media feed containing both organic and sponsored posts. Whereas the set of organic and sponsored posts was the same for all participants, we randomized the sequence in which participants were exposed to these posts between subjects. 

The feed featured thirty-five organic posts and five consumer electronics ads (sponsored posts; see @fig-design-2^[The figure displays five exemplary feeds used in Case Study 2. The positioning of sponsored and organic content in the study was fully randomized. The sponsored posts are highlighted for expositional clarity. Each feed contained identical content: the same thirty-five organic posts and five sponsored posts, as shown in the second column, where both the BOSE and Apple ads are displayed. An example feed is accessible at https://tiny.cc/DICE2.] for example feeds). We selected multiple ads from established consumer electronics brands (i.e., Apple, Bose, Nintendo, Samsung, Whoop).^[This approach aligns with recent methodological work on stimulus sampling [@SimonsohnEtAl_2024] which demonstrates that in studies focused on a single manipulated variable (a sponsored post's position in our case), using diverse stimuli helps ensure effects are not driven by idiosyncratic characteristics of any particular advertisement, increasing both internal and ecological validity.] The sponsored posts were actual ads retrieved from Facebook’s Ad Library, a publicly accessible database that archives advertisements run by advertisers across Meta’s platforms. The selected sponsored posts shared similar basic characteristics, including the presence of a product image, brand logo, and brief text as part of the advertisement. The organic content featured a collection of memes taken from the platform 9gag, a popular internet meme collection with over sixteen million followers on X. We intentionally chose memes as organic content to offer a conservative test of how branded content “competes” for attention on social media, particularly among younger users [@MalodiaEtAl_2022]. 


![Exemplary DICE Feeds from Case Study 2](figures-cs1-design.png){#fig-design-2}


### Procedure

```{r device_type_shared_1}
N <- subset[, length(unique(participant_label))]
share_desktop <- round(100 * subset[device_type == "Desktop",
                                    length(unique(participant_label))] / N)
share_mobile  <- round(100 * subset[device_type == "Mobile",
                                    length(unique(participant_label))] / N)
share_tablet  <- round(100 * subset[device_type == "Tablet",
                                    length(unique(participant_label))] / N)
```

We recruited `r data[, length(unique(participant_label))]` younger American participants from Prolific ($M_{age}$=`r data[, sprintf(fmt = "%.2f", mean(age, na.rm = TRUE))]` years; `r data[, round(100*mean(female, na.rm = TRUE))]`% female). Participants browsed the simulated feed on their own devices (`r share_desktop`% desktop, `r share_mobile`% mobile, and `r share_tablet`% tablet). After scrolling through the feed, we redirected participants to a Qualtrics survey in which participants first provided demographic information as a filler task. Next, we measured whether participants recalled seeing the ads by the five brands in the feed. Specifically, we measured both free and cued recall. To measure cued recall, we showed participants a list of twenty brands from different categories and asked them to indicate whether they recalled seeing them [@CampbellKeller_2003; @SimonovVallettiVeiga_2025], including a no-recall option. The results for both recall measures were highly consistent; we report the cued recall results in the manuscript for parsimony (the results for unaided recall are reported in @sec-uncued).


### Stimuli

We next illustrate how we configured the feed to match our experimental design. Specifically, we created a CSV file that contains forty rows where each row represents one unique post. To guarantee that the order in which the posts were displayed was randomized between participants, we left the `<sequence>` column empty. Whenever the DICE app encounters missing values in that column, it assigns random numbers to that cell. Hence, leaving some, or in our case, _all_ cells of this column empty, leads to random numbers and thus, random sequences in which the posts are displayed. Next, we specified the Boolean `<sponsored>` column and assigned a 0 to all thirty-five organic posts and a 1 to the five sponsored posts. For these sponsored posts, we also specified a `<target>` which is the URL of landing page participants are directed to if they click on the corresponding sponsored post. As a final step, we uploaded the CSV file to an online repository to create a URL that can be passed to DICE's web app.

@tbl-stimuli-case-1 shows an excerpt of the exact CSV files we used to create the stimuli for this study. You can download that file [here](https://raw.githubusercontent.com/Howquez/DICE/refs/heads/main/studies/meme_feed/stimuli/9gag.csv){target="_blank"}.

```{r}
#| label: tbl-stimuli-case-2 
#| tbl-cap: CSV File Used in Study 2

text_columns <- names(stimuli_2)[sapply(stimuli_2, is.character)]

# Apply the line break replacement, and also remove @ symbols and <br> tags
stimuli_2[, (text_columns) := lapply(.SD, function(x) {
  x <- str_replace_all(x, "\n(?=[^.,!?;:])", " ")
  x <- str_replace_all(x, "\n(?=[.,!?;:])", "")
  x <- str_replace_all(x, "@", "")
  str_replace_all(x, "<br>", " ")
}), .SDcols = text_columns]

kable(tail(x = stimuli_2, n = 7))
```


### Data and Randomization Checks

#### Final Sample

Our dataset comprises `r subset[, length(unique(participant_label))]` participants and `r format(x = subset[, .N], big.mark = ",")` observations at the participant $\times$ post level. In our analyses, we only focus on sponsored posts which is why our final sample comprises `r format(x = subset[sponsored == 1 & is.finite(log_dwell_pixel) & displayed_sequence < 39 & displayed_sequence > 2, .N], big.mark = ",")` observations on the participant $\times$ _sponsored_ post level. This is less than the expected five observations per participant due to two reasons. First, due to connectivity issues: no dwell time data were recorded for around `r sprintf(fmt = "%.2f", 100 * (data[sponsored == 1 & !is.finite(log_dwell_pixel), .N]/ data[sponsored == 1, .N]))`% of individual–sponsored post pairs. Second, we excluded the first and last two posts of each feed (i.e., $300 \times 4$ observations) from our analysis, as meaningful dwell times couldn't be determined for these. This is because participants were familiarizing themselves with the interface at the start and deciding whether to proceed to the next stage of the study at the end of the feeds.

#### Variables

Position in feed (subsequently referred to only as “position”) is our main independent variable. Because we randomized the order in which the content appeared (i.e., position acts as a within-subject factor) and because we excluded observations positioned at the beginning and the end of the feed, we observe a sample mean of `r sprintf(fmt = "%.2f",  subset[sponsored == 1, mean(displayed_sequence)])` as well as a minimum and maximum of `r round(subset[sponsored == 1, min(displayed_sequence)])` and `r round(subset[sponsored == 1, max(displayed_sequence)])`, respectively. As, we randomly manipulated the position in which each post was displayed exogenously between subjects, each sequence in which participants browsed through ads was unique.

```{r}
#| eval: true
#| label: fig-brand-order
#| fig-cap: "Distribution of Sponsored Post Impressions by Position in Feed and Brand across All Participants"
#| fig-cap-location: top

tmp <- subset[sponsored == TRUE, 
            .(N = .N), 
            by = c("displayed_sequence", "brand")][order(brand, displayed_sequence)]
ggplot(data = tmp, 
       mapping = aes(x = displayed_sequence, y = N)) +
  facet_grid(rows = vars(brand)) +
  geom_line() +
  # geom_smooth(method = "loess", col = NA) +
  scale_y_continuous(limits = c(0, 20), expand = c(0, 0, 0.1, 0), breaks = c(0, 7.5, 15)) +
  scale_x_continuous(limits = c(1, 40), expand = c(0, 0, 0, 0), breaks = c(1, 3, 10, 20, 30, 38, 40)) +
  geom_hline(yintercept = 37.5/5, alpha = 0.5, lty = 2) +
  geom_hline(yintercept = 0, alpha = 0.75, linewidth = 1) +
  geom_vline(xintercept = 3, alpha = 0.25) +
  geom_vline(xintercept = 38, alpha = 0.25) +
  layout +
  labs(y = "Number of Sponsored Impressions", x = "Position") +
  theme(panel.grid.major.y = element_blank(),
        strip.background =element_rect(fill="#FFFFFF"),
        axis.line.x = element_blank())
```

In @fig-brand-order, each line represents the number of times a sponsored post for a specific brand appeared at each position across all participants. As the placements were fully randomized, we observed some random variability that naturally fluctuates around the expected value of 7.5 impressions per position (as indicated by dashed lines).^[This expectation is based on each of the 5 sponsored posts being shown once per participant (N=300) in a feed of 40 positions, leading to an average of $\frac{300}{40} = 7.5$ for each brand's ad placement across all positions.] Taken together, this suggests that randomization within the DICE app was effective.

We measured dwell times as the number of seconds at least 50 percent of a post’s pixels were visible on screen. We log-transformed the raw dwell times to reduce skewness. Unlike Case Study 2, where we focused on a single post (i.e., the KLM ad), the focal posts of interest (i.e., ads) in this Case Study vary in their post height. Thus, and as described in the “Behavioral Data” section of the DICE App Implementation, we normalized our dwell time measure by dividing it by post height to control for differently sized posts (i.e., the height in pixels of the corresponding sponsored post on a participant’s screen).

We also tracked actual reactions to the content such as likes and replies to individual posts. In the full sample featuring both organic and sponsored posts, we observed
`r subset[has_liked_any == TRUE, length(unique(participant_label))]`
(`r subset[has_replied_any == TRUE, length(unique(participant_label))]`)
participants who liked (replied to) any post in the feed. These numbers are obviously lower for sponsored posts (ads shown in the feed) with 
`r subset[has_liked_sponsored == TRUE, length(unique(participant_label))]`
(`r subset[has_replied_sponsored == TRUE, length(unique(participant_label))]`)
participants liked (replied to) at least one sponsored post or ad. Given the low incident rate, we did not analyze these likes and comments any further.

@tbl-show-data-1 shows an excerpt of the processed data to illustrate its nested (i.e., "long") structure.

```{r}
#| label: tbl-show-data-1
#| tbl-cap: Processed Data Analyzed in Study 1

tmp <- data[,
            .(`Participant ID` = participant_label,
              `Post ID` = doc_id,
              `Sponsored Post` = sponsored,
              Brand = brand,
              `Position in Feed` = displayed_sequence,
              Likes = liked,
              Replies = hasReply,
              `Seconds in Viewport` = seconds_in_viewport,
              `log(Seconds in Viewport)` = log_dwell_time,
              `Post Height` = height,
              `Dwell Time` = log_dwell_pixel,
              Age = age, 
              Female = female, 
              Desktop = as.logical(is_desktop),
              `Recall Apple` = cued_apple, 
              `Recall Bose` = cued_bose, 
              `Recall Nintendo` = cued_nintendo, 
              `Recall Samsung` = cued_samsung, 
              `Recall Whoop` = cued_whoop)]

kable(tmp[35:45])
```


### Empirical Model

To estimate the impact of ad positioning on brand recall in social media feeds, we employed a mixed-effects logistic regression model with brand fixed effects to account for the binary nature of recall outcome (recalled vs. not recalled) while considering the hierarchical structure of our data: multiple observations nested within participants and ads. We assume a binomial distribution as each observation represents a single trial with two possible outcomes (recalled vs. not recalled), where $p_{ij}$ represents the probability of participant $i$ recalling brand $j$:

$$
\text{recall}_{ij} \sim \text{Binomial}(1, p_{ij})
$$

We estimated the effect of ad positioning on recall and captured between-participant heterogeneity through random intercepts while controlling for brand fixed effects:

$$
\text{logit}(p_{ij}) = a + a_i + \mathbf{x}_{ij} \mathbf{b} + \sum_{j=1}^{J-1} \gamma_j \text{Brand}_j
$$

where $a$ is the global intercept, $a_i$ is the participant-specific random intercept, $\mathbf{x}_{ij}$ is a vector of continuous predictors (e.g., position and dwell time) with corresponding coefficient vector $\mathbf{b}$, and $\gamma_j$ represents the fixed effects for each brand $j$ (with Apple serving as the reference category). The random participant effects $a_i$ follows from our experimental design, given that the random assignment of ad positions ensures zero correlation between participant characteristics and the explanatory variables. Brand effects are treated as fixed parameters rather than random effects, allowing for potential correlation between brand characteristics and positioning.

The random participant effects $a_i$ follows from our experimental design, given that the random assignment of ad positions ensures zero correlation between participant characteristics and the explanatory variables. Brand effects are treated as fixed parameters rather than random effects, allowing for potential correlation between brand characteristics and core explanatory variables such as dwell time.

Finally, for better comparability between models, we z-standardized all explanatory variables. The regression coefficients effects on the dependent variable are therefore quantified in standard deviations. This allowed us to compare the relative effect sizes between regression models. Because of the logit link, the odds ratio is $100 \times (e^{\beta} - 1)$, which gives the percentage change in the odds of recall.


### Results and Discussion

```{r empirical-models}
tmp <- copy(subset[sponsored == 1])
tmp[, log_dwell_pixel := scale(log_dwell_pixel)]
tmp[, displayed_sequence := scale(displayed_sequence)]


glmer_1 <- glmer(recalled_brand_cued ~ displayed_sequence + I(displayed_sequence^2) + 
                   (1|participant_label) + brand,
                 data = tmp,
                 family = binomial(link = "logit"))

glmer_2 <- glmer(recalled_brand_cued ~ log_dwell_pixel + 
                   (1|participant_label) + brand,
                 data = tmp,
                 family = binomial(link = "logit"))

glmer_3 <- glmer(recalled_brand_cued ~ displayed_sequence + I(displayed_sequence^2) + log_dwell_pixel + brand + 
                   (1|participant_label),
                 data = tmp,
                 family = binomial(link = "logit"),
                 control = glmerControl(optimizer = "optimx", 
                                      optCtrl = list(method = "nlminb")))
```

```{r model-statistics}
model_1_output <- summary(glmer_1)
  model_1_coefficients <- coef(model_1_output)
  model_1_beta_1 <- apa(model_1_coefficients[2, 1], decimals = 3, leading = FALSE)
  model_1_se_1   <- apa(model_1_coefficients[2, 2], decimals = 3, leading = FALSE)
  model_1_z_1    <- apa(model_1_coefficients[2, 3], decimals = 3, leading = FALSE)
  model_1_p_1    <- apa(model_1_coefficients[2, 4], decimals = 3, leading = FALSE)

model_2_output <- summary(glmer_2)
  model_2_coefficients <- coef(model_2_output)
  model_2_beta_3 <- apa(model_2_coefficients[2, 1], decimals = 3, leading = FALSE)
  model_2_se_3   <- apa(model_2_coefficients[2, 2], decimals = 3, leading = FALSE)
  model_2_z_3    <- apa(model_2_coefficients[2, 3], decimals = 3, leading = FALSE)
  model_2_p_3    <- apa(model_2_coefficients[2, 4], decimals = 3, leading = FALSE)
  
model_3_output <- summary(glmer_3)
  model_3_coefficients <- coef(model_3_output)
  model_3_beta_1 <- apa(model_3_coefficients[2, 1], decimals = 3, leading = FALSE)
  model_3_se_1   <- apa(model_3_coefficients[2, 2], decimals = 3, leading = FALSE)
  model_3_z_1    <- apa(model_3_coefficients[2, 3], decimals = 3, leading = FALSE)
  model_3_p_1    <- apa(model_3_coefficients[2, 4], decimals = 3, leading = FALSE)
  model_3_beta_3 <- apa(model_3_coefficients[4, 1], decimals = 3, leading = FALSE)
  model_3_se_3   <- apa(model_3_coefficients[4, 2], decimals = 3, leading = FALSE)
  model_3_z_3    <- apa(model_3_coefficients[4, 3], decimals = 3, leading = FALSE)
  model_3_p_3    <- apa(model_3_coefficients[4, 4], decimals = 3, leading = FALSE)
```

The effect of _position_ on recall was significant and negative
($\beta_1$ = `r model_1_beta_1`, 
SE = `r model_1_se_1`,
z = `r model_1_z_1`,
p = `r model_1_p_1`;
see Model 1 in @tbl-mixed-effects), suggesting a primacy effect such that the further up (down) an ad is displayed in a feed, the more (less) participants recall seeing the ad. We also examined potential non-linear effects of position (i.e., to assess whether especially the beginning and end of a feed promote ad recall) by adding a quadratic term, but found no statistically significant effect. These results are robust to alternative model specifications, using _uncued_ recall (instead of cued recall) as our dependent variable (see @sec-uncued). 

```{r hierarchical-mediation}
tmp <- copy(subset[sponsored == 1])
tmp[, log_dwell_pixel := scale(log_dwell_pixel)]
tmp[, displayed_sequence := scale(displayed_sequence)]


med_model <- lmer(log_dwell_pixel ~ displayed_sequence + brand + (1|participant_label), 
                  data = tmp)

out_model <- glmer(recalled_brand_cued ~ displayed_sequence + brand + log_dwell_pixel + 
                   (1|participant_label), 
                   data = tmp, 
                   family = binomial(link = "logit"),
                   control = glmerControl(optimizer = "optimx",
                                          optCtrl = list(method = "nlminb")))


# check convergence
# summary(med_model)
# summary(out_model)

# mediation analysis
med_results <- mediate(med_model, out_model, 
                      treat = "displayed_sequence",
                      mediator = "log_dwell_pixel",
                      boot = FALSE, sims = 1000)

# plot(med_results)
```

```{r 1-1-1-statistics}
model_summary <- summary(med_results)

acme_estimate <- apa(model_summary$d.avg, decimals = 3, leading = FALSE)
acme_ci_lower <- apa(model_summary$d.avg.ci[1], decimals = 3, leading = FALSE)
acme_ci_upper <- apa(model_summary$d.avg.ci[2], decimals = 3, leading = FALSE)
prop_med <- apa(model_summary$n.avg, decimals = 3, leading = FALSE)
prop_med_ci_lower <- apa(model_summary$n.avg.ci[1], decimals = 3, leading = FALSE)
prop_med_ci_upper <- apa(model_summary$n.avg.ci[2], decimals = 3, leading = FALSE)
total_effect_p <- apa(model_summary$tau.p, decimals = 3, leading = FALSE)
```

As shown in Model 2 in @tbl-mixed-effects, the post-level dwell time of a user significantly predicts ad recall
($\beta_3$ = `r model_2_beta_3`, 
SE = `r model_2_se_3`,
z = `r model_2_z_3`,
p = `r model_2_p_3`).
More importantly, we also found that the dwell time allocated to an ad was a stronger predictor of recall than the position of the ad in the feed. Specifically, Model 3, including both position and dwell time, shows that the effect of position becomes non-significant 
($\beta_1$ = `r model_3_beta_1`, 
SE = `r model_3_se_1`,
z = `r model_3_z_1`,
p = `r model_3_p_1`),
while the dwell time coefficient remains unchanged and significant 
($\beta_3$ = `r model_3_beta_3`, 
SE = `r model_3_se_3`,
z = `r model_3_z_3`,
p = `r model_3_p_3`). 
The minimal change in the Akaike information criterion when adding the position coefficients between Model 2 and Model 3 suggests that including ad position does not improve the model’s fit to the data. A subsequent 1-1-1 hierarchical mediation analysis [@ZhangEtAl_2008] indeed supports that post-level dwell time significantly mediates the relationship between ad position and recall
($a \times b = `r acme_estimate`$, 
95% CI = [`r acme_ci_lower`, `r acme_ci_upper`], 
proportion mediated = `r prop_med`, 
95% CI = [`r prop_med_ci_lower`, `r prop_med_ci_upper`], 
p = 
`r total_effect_p`).
@sec-brands reports additional analyses that suggest limited heterogeneity in these patterns across different sponsored posts. While brands may differ in their baseline dwell times due to potential familiarity differences, dwell time consistently predicts recall across brands.


```{r}
#| tbl-cap: Estimates of Recall as a Function of Ad Position and Dwell Time
#| results: asis
#| label: tbl-mixed-effects
n_obs <- format(nobs(glmer_1), big.mark = ",")

tbl_1 <- tbl_regression(
  glmer_1, 
  intercept = TRUE,  # This ensures the intercept is included
  include = c(displayed_sequence, `I(displayed_sequence^2)`),
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE,
  label = list(
    displayed_sequence ~ "Position",
    `I(displayed_sequence^2)` ~ "Position²"
  ),
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = AIC)

tbl_2 <- tbl_regression(
  glmer_2, 
  intercept = TRUE,
  include = c(log_dwell_pixel),
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE,
  label = list(
    log_dwell_pixel ~ "Dwell Time"
  ),
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = AIC)

tbl_3 <- tbl_regression(
  glmer_3, 
  intercept = TRUE,
  include = c(displayed_sequence, `I(displayed_sequence^2)`, log_dwell_pixel),
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE,
  label = list(
    displayed_sequence ~ "Position",
    `I(displayed_sequence^2)` ~ "Position²",
    log_dwell_pixel ~ "Dwell Time"
  ),
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = AIC)

table <- tbl_merge(
  tbls = list(tbl_1, tbl_2, tbl_3),
  tab_spanner = c("Model 1", "Model 2", "Model 3")
) |>
  modify_table_body(
    ~ .x |>
      dplyr::arrange(
        factor(label, levels = c("(Intercept)", "Position", "Position²", "Dwell Time", "AIC"))
      )
  ) |>
  modify_header(
    label ~ "",
    estimate_1 ~ "beta",
    estimate_2 ~ "beta",
    estimate_3 ~ "beta",
    std.error_1 ~ "SE",
    std.error_2 ~ "SE",
    std.error_3 ~ "SE",
    p.value_1 ~ "p",
    p.value_2 ~ "p",
    p.value_3 ~ "p"
  )

table
```

From a substantive perspective, the findings demonstrate how memory formation operates when brands compete for limited attention in a social media environment, consistent with prior work on attention and memory under competitive exposure [@PietersWarlopWedel_2002]. From a more methodological perspective, Case Study 2 demonstrates how to combine the behavioral data (i.e., post-level dwell time as a proxy of attention) with self-reports from conventional survey measures (e.g., recall of branded content).


### Robustness Checks {#sec-robustness-checks-2}

#### Uncued Recall {#sec-uncued}

Here, we reproduce @tbl-mixed-effects using uncued (or "free") instead of cued recall as our dependent variable, using the same empirical strategy as before. 

@tbl-mixed-effects-uncued reproduces these findings and shows that they are robust to this alternative model specification: we still observe a primacy effect in out naïve Model 1 that vanishes as we control for dwell time in Model 3.

```{r empirical-models-uncued}
tmp <- copy(subset[sponsored == 1])
tmp[, log_dwell_pixel := scale(log_dwell_pixel)]
tmp[, displayed_sequence := scale(displayed_sequence)]


glmer_1 <- glmer(recalled_brand_uncued ~ displayed_sequence + I(displayed_sequence^2) + 
                   (1|participant_label) + brand,
                 data = tmp,
                 family = binomial(link = "logit"))

glmer_2 <- glmer(recalled_brand_uncued ~ log_dwell_pixel + 
                   (1|participant_label) + brand,
                 data = tmp,
                 family = binomial(link = "logit"))

glmer_3 <- glmer(recalled_brand_uncued ~ displayed_sequence + I(displayed_sequence^2) + log_dwell_pixel + brand + 
                   (1|participant_label),
                 data = tmp,
                 family = binomial(link = "logit"),
                 control = glmerControl(optimizer = "optimx", 
                                      optCtrl = list(method = "nlminb")))
```

```{r model-statistics-uncued}
model_1_output <- summary(glmer_1)
  model_1_coefficients <- coef(model_1_output)
  model_1_beta_1 <- apa(model_1_coefficients[2, 1], decimals = 3, leading = FALSE)
  model_1_se_1   <- apa(model_1_coefficients[2, 2], decimals = 3, leading = FALSE)
  model_1_z_1    <- apa(model_1_coefficients[2, 3], decimals = 3, leading = FALSE)
  model_1_p_1    <- apa(model_1_coefficients[2, 4], decimals = 3, leading = FALSE)

model_2_output <- summary(glmer_2)
  model_2_coefficients <- coef(model_2_output)
  model_2_beta_3 <- apa(model_2_coefficients[2, 1], decimals = 3, leading = FALSE)
  model_2_se_3   <- apa(model_2_coefficients[2, 2], decimals = 3, leading = FALSE)
  model_2_z_3    <- apa(model_2_coefficients[2, 3], decimals = 3, leading = FALSE)
  model_2_p_3    <- apa(model_2_coefficients[2, 4], decimals = 3, leading = FALSE)
  
model_3_output <- summary(glmer_3)
  model_3_coefficients <- coef(model_3_output)
  model_3_beta_1 <- apa(model_3_coefficients[2, 1], decimals = 3, leading = FALSE)
  model_3_se_1   <- apa(model_3_coefficients[2, 2], decimals = 3, leading = FALSE)
  model_3_z_1    <- apa(model_3_coefficients[2, 3], decimals = 3, leading = FALSE)
  model_3_p_1    <- apa(model_3_coefficients[2, 4], decimals = 3, leading = FALSE)
  model_3_beta_3 <- apa(model_3_coefficients[4, 1], decimals = 3, leading = FALSE)
  model_3_se_3   <- apa(model_3_coefficients[4, 2], decimals = 3, leading = FALSE)
  model_3_z_3    <- apa(model_3_coefficients[4, 3], decimals = 3, leading = FALSE)
  model_3_p_3    <- apa(model_3_coefficients[4, 4], decimals = 3, leading = FALSE)
```

The effect of position on _uncued_ recall was significant and negative
($\beta_1$ = `r model_1_beta_1`, 
SE = `r model_1_se_1`,
z = `r model_1_z_1`,
p = `r model_1_p_1`;
see Model 1 in @tbl-mixed-effects-uncued), suggesting a primacy effect such that the further up (down) an ad is displayed in a feed, the more (less) participants recall seeing the ad. We also examined potential non-linear effects of position (i.e., to assess whether especially the beginning and end of a feed promote uncued ad recall) by adding a quadratic term, but found no statistically significant effect.


```{r hierarchical-mediation-uncued}
tmp <- copy(subset[sponsored == 1])
tmp[, log_dwell_pixel := scale(log_dwell_pixel)]
tmp[, displayed_sequence := scale(displayed_sequence)]


med_model <- lmer(log_dwell_pixel ~ displayed_sequence + brand + (1|participant_label), 
                  data = tmp)

out_model <- glmer(recalled_brand_uncued ~ displayed_sequence + brand + log_dwell_pixel + 
                   (1|participant_label), 
                   data = tmp, 
                   family = binomial(link = "logit"),
                   control = glmerControl(optimizer = "optimx",
                                          optCtrl = list(method = "nlminb")))


# check convergence
# summary(med_model)
# summary(out_model)

# mediation analysis
med_results <- mediate(med_model, out_model, 
                      treat = "displayed_sequence",
                      mediator = "log_dwell_pixel",
                      boot = FALSE, sims = 1000)

# plot(med_results)
```

```{r 1-1-1-statistics-uncued}
model_summary <- summary(med_results)

acme_estimate <- apa(model_summary$d.avg, decimals = 3, leading = FALSE)
acme_ci_lower <- apa(model_summary$d.avg.ci[1], decimals = 3, leading = FALSE)
acme_ci_upper <- apa(model_summary$d.avg.ci[2], decimals = 3, leading = FALSE)
prop_med <- apa(model_summary$n.avg, decimals = 3, leading = FALSE)
prop_med_ci_lower <- apa(model_summary$n.avg.ci[1], decimals = 3, leading = FALSE)
prop_med_ci_upper <- apa(model_summary$n.avg.ci[2], decimals = 3, leading = FALSE)
total_effect_p <- apa(model_summary$tau.p, decimals = 3, leading = FALSE)
```

As shown in Model 2 in @tbl-mixed-effects-uncued, the post-level dwell time of a user significantly predicts uncued recall
($\beta_3$ = `r model_2_beta_3`, 
SE = `r model_2_se_3`,
z = `r model_2_z_3`,
p = `r model_2_p_3`).
More importantly, we also found that the dwell time allocated to an ad was a stronger predictor of uncued recall than the position of the ad in the feed. Specifically, Model 3, including both position and dwell time, shows that the effect of position becomes non-significant 
($\beta_1$ = `r model_3_beta_1`, 
SE = `r model_3_se_1`,
z = `r model_3_z_1`,
p = `r model_3_p_1`),
while the dwell time coefficient remains unchanged and significant 
($\beta_3$ = `r model_3_beta_3`, 
SE = `r model_3_se_3`,
z = `r model_3_z_3`,
p = `r model_3_p_3`). 
The minimal change in the Akaike information criterion when adding the position coefficients between Model 2 and Model 3 suggests that including ad position does not improve the model’s fit to the data. A subsequent 1-1-1 hierarchical mediation analysis [@ZhangEtAl_2008] indeed supports that post-level dwell time significantly mediates the relationship between ad position and uncued recall
($a \times b = `r acme_estimate`$, 
95% CI = [`r acme_ci_lower`, `r acme_ci_upper`], 
proportion mediated = `r prop_med`, 
95% CI = [`r prop_med_ci_lower`, `r prop_med_ci_upper`], 
p = 
`r total_effect_p`).


```{r}
#| tbl-cap: Estimates of Uncued Recall as a Function of Ad Position and Dwell Time
#| results: asis
#| label: tbl-mixed-effects-uncued
n_obs <- format(nobs(glmer_1), big.mark = ",")

tbl_1 <- tbl_regression(
  glmer_1, 
  intercept = TRUE,  # This ensures the intercept is included
  include = c(displayed_sequence, `I(displayed_sequence^2)`),
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE,
  label = list(
    displayed_sequence ~ "Position",
    `I(displayed_sequence^2)` ~ "Position²"
  ),
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = AIC)

tbl_2 <- tbl_regression(
  glmer_2, 
  intercept = TRUE,
  include = c(log_dwell_pixel),
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE,
  label = list(
    log_dwell_pixel ~ "Dwell Time"
  ),
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = AIC)

tbl_3 <- tbl_regression(
  glmer_3, 
  intercept = TRUE,
  include = c(displayed_sequence, `I(displayed_sequence^2)`, log_dwell_pixel),
  estimate_fun = ~ style_number(.x, digits = 3),
  pvalue_fun = label_style_pvalue(digits = 3, zero.print = "."),
  conf.int = FALSE,
  label = list(
    displayed_sequence ~ "Position",
    `I(displayed_sequence^2)` ~ "Position²",
    log_dwell_pixel ~ "Dwell Time"
  ),
  add_estimate_to_reference_rows = FALSE
) |>
  modify_column_unhide(columns = std.error) |>
  add_glance_table(include = AIC)

table <- tbl_merge(
  tbls = list(tbl_1, tbl_2, tbl_3),
  tab_spanner = c("Model 1", "Model 2", "Model 3")
) |>
  modify_table_body(
    ~ .x |>
      dplyr::arrange(
        factor(label, levels = c("(Intercept)", "Position", "Position²", "Dwell Time", "AIC"))
      )
  ) |>
  modify_header(
    label ~ "",
    estimate_1 ~ "beta",
    estimate_2 ~ "beta",
    estimate_3 ~ "beta",
    std.error_1 ~ "SE",
    std.error_2 ~ "SE",
    std.error_3 ~ "SE",
    p.value_1 ~ "p",
    p.value_2 ~ "p",
    p.value_3 ~ "p"
  )

table
```


#### Ad Position, Dwell Time, and Recall Across Brands {#sec-brands}

Ad recall significantly varied across brands. @fig-by-brand (Panel A) visualizes the relationship between dwell time and position across the five brands in our study. We observe a consistent negative relationship between position and dwell time across all brands, with posts placed later in the feed receiving less attention. 

```{r}
p4 <- ggplot(data = subset[sponsored == 1], 
       mapping = aes(x = displayed_sequence, y = log_dwell_pixel, 
                     col = brand)) +
  geom_smooth(method = "lm", formula = "y ~ x", se = FALSE) +
  coord_cartesian(ylim = c(-0.001, 0.005)) +
  scale_x_continuous(limits = c(1, 40), expand = c(0, NA), breaks = c(3, 10, 20, 30, 38)) +
  geom_vline(xintercept = 3, alpha = 0.25) +
  geom_vline(xintercept = 38, alpha = 0.25) +
  scale_colour_tron() +
  layout +
  labs(title = "B", y = "Dwell Time", x = "Position", col = "Brand")
```

```{r}
p6 <- ggplot(data = subset[sponsored == 1], 
       mapping = aes(x = log_dwell_pixel, y = as.numeric(recalled_brand_cued),
                     col = brand)) +
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"),
              formula = "y ~ x", se = FALSE) +
  scale_colour_tron() +
  scale_x_continuous(breaks = c(-0.02, 0, 0.02)) +
  layout +
  labs(title = "C", y = "Recall", x = "Dwell Time", col = "Brand")
```


```{r}
#| label: fig-by-brand
#| fig-cap: "Ad Position, Dwell Time, and Recall Across Brands"
#| fig-cap-location: top

(p4 + p6 + plot_layout(guides = "collect")) & theme(legend.position = "bottom")

```

However, we also find that Apple and Nintendo generated significantly higher dwell time than all other brands (see the parallel upward shift of the regression line compared to Bose, Samsung, and Whoop). Panel B further shows how dwell time predicts recall probability consistently across all brands. The positive slope of all curves in Panel B indicates that increased dwell time enhances recall probability across all brands consistently, even for less familiar brands like Whoop. This suggests that while brands may differ in their baseline dwell time due to potential familiarity differences, we find a highly consistent empirical regularity of how dwell time ultimately predicts recall.

