# Case Studies {#sec-cases}

## Case Study 1: When Do Users Remember Ads? Dwell Time Outperforms Position in Explaining Recall {#sec-positioning-case}

```{r install_packages}
#| warning: false
#| output: false

options(repos = c(CRAN = "https://cran.r-project.org")) 


if (!requireNamespace("groundhog", quietly = TRUE)) {
    install.packages("groundhog")
    library("groundhog")
}

pkgs <- c("magrittr", "data.table", "knitr", "kableExtra", "stringr", "english", "moments",
          "ggplot2", "patchwork", "scales",  "ggdist", "gghalves", "sjPlot", "gtsummary", "wesanderson", "ggsci",
          "stargazer", "gt",
          "lme4")

groundhog::groundhog.library(pkg = pkgs,
                             date = "2024-10-01")

rm(pkgs)
```


```{r layout}
#| echo: false
layout <- theme(panel.background = element_rect(fill = "white"),
                legend.key = element_rect(fill = "white"),
                panel.grid.major.y = element_line(colour = "grey", 
                                                  linewidth = 0.25),
                axis.ticks.y = element_blank(),
                panel.grid.major.x = element_blank(),
                axis.line.x.bottom = element_line(colour = "#000000", 
                                                  linewidth = 0.5),
                axis.line.y.left = element_blank(),
                plot.title = element_text(size = rel(1))
)
```

```{r colors}
#| echo: false
c_orange <- "#F0941F"
c_teal   <- "#196774"

c_positive <- "#377E39"
c_negative <- "#7D3756"

scale_color_custom_d <- function() {
  scale_color_manual(values = c(c_orange, c_teal))
}

scale_fill_custom_d <- function() {
  scale_fill_manual(values = c(c_orange, c_teal))
}

scale_color_custom_2d <- function() {
  scale_color_manual(values = c(c_positive, c_negative))
}

scale_fill_custom_2d <- function() {
  scale_fill_manual(values = c(c_positive, c_negative))
}
```

```{r seed}
#| echo: false
set.seed(42)
```

```{r read_meme_data}
data <- fread(file = "../../oFeeds/studies/meme_feed/data/processed/meme-feed-data.csv", na.strings = "")
stimuli_1  <- fread(file = "../templates/docs/data/case-study-1/9gag.csv", na.strings = "")
```

```{r subset_meme_data}
subset <- data[is.finite(log_dwell_pixel) & 
                     displayed_sequence < 39 & 
                     displayed_sequence > 2]
```


While investigating which content _cuts through the clutter_ [@Ordenes_2019], we focus on the content's position rather than examining a sponsored post's characteristics [see, e.g., @BergerMoeSchweidel_2023 who study the content's linguistic features]. Specifically, we examine whether a sponsored post's position in a social media feed affects its recall, building on research about recency and primacy effects [see, e.g., @WedelPieters_2000; @MurphyHofackerMizerski_2006; @AgarwalHosanagarSmith_2011]. A naïve model reveals a primacy effect, where sponsored posts in higher positions show increased likelihood of recall. However, this effect disappears when controlling for dwell time. Furthermore, our analysis demonstrates that longer dwell times correlate with higher recall probability for sponsored posts.

Using position effects in social media advertising as an illustrative example, the primary goal of this case study is to demonstrate practical applications of the DICE app's dwell time measurement. By showing how to create a feed and then link post-specific dwell times with self-reported recall measures, we provide a practical template for researchers who intend to combine DICE's behavioral measures with survey data elicited in tools such as Qualtrics.

### Experimental Design

To investigate the relationship between ad placement and recall, we simulated  [social media feeds](https://perma.cc/87B9-H6ZV){target="_blank"} containing both organic and sponsored posts. Whereas the set of organic and sponsored posts was the same for all participants, the sequence in which the participants were exposed to these posts was unique for every participant as we randomized the sequence between subjects.

Aiming to create ecologically valid stimuli, we populated our experimental feed with thrity-five memes (organic posts) and five consumer electronics advertisements (sponsored posts). Accordingly, we collected memes from _9gag_, a popular account with more than sixteen million followers on X (formerly called "Twitter") that is well known for its internet meme collections. The choice of memes as organic content was deliberate, given their dominant role in social media engagement, particularly among younger users: @MalodiaEtAl_2022 report that 75% of social media users aged 13 to 36 regularly share memes, and 30% of these users share memes daily, with Instagram users sharing over 1 million meme-related posts per day in 2020 [@Instagram_2020]. This prevalence of meme consumption among younger demographics informed both our recruiting strategy and our selection of sponsored content: we recruited 
 `r data[, length(unique(participant_label))]`
young American participants ($M_{age}$=`r data[, sprintf(fmt = "%.2f", mean(age, na.rm = TRUE))]` years; `r data[, round(100*mean(female, na.rm = TRUE))]`% female) from Prolific and selected consumer electronics advertisements from established brands (i.e., Apple, Bose, Nintendo, Samsung, Whoop) retrieved from Facebook's Ad Library--a publicly accessible database that archives advertisements running across Meta's platforms. The selected sponsored posts showed natural variation in their characteristics, reflecting the diversity of ads users encounter in their daily social media use. This approach aligns with recent methodological work on stimulus sampling [@SimonsohnEtAl_2024] which demonstrates that in studies focused on a single manipulated variable (a sponsored post's position in our case), using diverse stimuli helps ensure effects are not driven by idiosyncratic characteristics of any particular advertisement, increasing both internal and ecological validity.

```{r}
sample_n <- 15
```

@fig-design provides a visual representation of our experimental design, specifically illustrating the randomization of sponsored post positions across participants. The figure plots the position in feed against `r as.english(sample_n)` randomly sampled participants, with different colors representing the five brands. @fig-design demonstrates how the sponsored posts' positions were randomly distributed throughout the feed, ensuring that each participant encountered the five advertisements in a unique sequence with no systematic patterns in the placement of any particular brand's advertisements.

```{r}
#| results: asis
#| warning: false
#| label: fig-design 
#| fig-cap: Randomized Placement of Sponsored Posts Across Participants' Feeds

sample_n <- 15
sampled_ids <- sample(x = data[, unique(participant_label)],
                      size = sample_n, 
                      replace = FALSE)
ggplot(data = data[!is.na(brand) & participant_label %in% sampled_ids],
       mapping = aes(x = participant_label, y = displayed_sequence, fill = brand)) +
  geom_hline(yintercept = 1) +
  geom_hline(yintercept = 40) +
  geom_tile() +
  labs(y = "Position in Feed", x = "Sampled Participants", fill = "Brand") +
  scale_x_discrete(position = "top", labels = seq(from = 1, to = sample_n, by = 1)) +
  scale_y_reverse(limits = c(41, 0), expand = c(0, NA), breaks = c(1, 20, 40)) +
  # scale_fill_manual(values = wes_palette("FantasticFox1")) +
  scale_fill_tron() +
  layout +
  theme(# axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major.y = element_blank(),
        legend.position = "bottom")
```

### Procedure

```{r device_type_shared_1}
N <- subset[, length(unique(participant_label))]
share_desktop <- round(100 * subset[device_type == "Desktop",
                                    length(unique(participant_label))] / N)
share_mobile  <- round(100 * subset[device_type == "Mobile",
                                    length(unique(participant_label))] / N)
share_tablet  <- round(100 * subset[device_type == "Tablet",
                                    length(unique(participant_label))] / N)
```

Participants browsed the simulated feed on their own devices
(`r share_desktop`% Desktop, `r share_mobile`% Mobile, and `r share_tablet`% Tablet), allowing for unobtrusive measurement of dwell time for each post. 
After scrolling through the feed, we redirected participants to a Qualtrics survey in which they first provided demographic information as a filler task. Subsequently, we measured whether participants recalled seeing the ads by the five brands in the feed. Specifically, we measured cued recall for which we showed participants a list of twenty brands from different categories and asked them to indicate whether they recalled seeing them [see, e.g., @CampbellKeller_2003; @SimonovVallettiVeiga_2024]. We also included a no-recall option. Finally, participants read a debriefing and were redirected to Prolific.


### Stimuli

We next illustrate how we configured the feed to match our experimental design. Specifically, we created a CSV file that contains forty rows where each row represents one unique post. To guarantee that the order in which the posts were displayed was randomized between participants, we left the `<sequence>` column empty. Whenever the DICE app encounters missing values in that column, it assigns random numbers to that cell. Hence, leaving some, or in our case, _all_ cells of this column empty, leads to random numbers and thus, random sequences in which the posts are displayed. Next, we specified the Boolean `<sponsored>` column and assigned a 0 to all thirty-five organic posts and a 1 to the five sponsored posts. For these sponsored posts, we also specified a `<target>` which is the URL of landing page participants are directed to if they click on the corresponding sponsored post. As a final step, we uploaded the CSV file to an online repository to create a URL that can be passed to DICE's web app.

@tbl-stimuli-case-1 shows an excerpt of the exact CSV files we used to create the stimuli for this study. You can download that file [here](https://raw.githubusercontent.com/Howquez/DICE/refs/heads/main/studies/meme_feed/stimuli/9gag.csv){target="_blank"}.

```{r}
#| label: tbl-stimuli-case-1 
#| tbl-cap: CSV File Used in Study 1

text_columns <- names(stimuli_1)[sapply(stimuli_1, is.character)]

# Apply the line break replacement, and also remove @ symbols and <br> tags
stimuli_1[, (text_columns) := lapply(.SD, function(x) {
  x <- str_replace_all(x, "\n(?=[^.,!?;:])", " ")
  x <- str_replace_all(x, "\n(?=[.,!?;:])", "")
  x <- str_replace_all(x, "@", "")
  str_replace_all(x, "<br>", " ")
}), .SDcols = text_columns]

kable(tail(x = stimuli_1, n = 7))
```


### Data

#### Final Sample

Our dataset comprises `r subset[, length(unique(participant_label))]` participants and `r format(x = subset[, .N], big.mark = ",")` observations at the participant $\times$ post level. In our analyses, we only focus on sponsored posts which is why our final sample comprises `r format(x = subset[sponsored == 1 & is.finite(log_dwell_pixel) & displayed_sequence < 39 & displayed_sequence > 2, .N], big.mark = ",")` observations on the participant $\times$ _sponsored_ post level. This is less than the expected five observations per participant due to two reasons. First, due to connectivity issues: no dwell time data were recorded for around `r sprintf(fmt = "%.2f", 100 * (data[sponsored == 1 & !is.finite(log_dwell_pixel), .N]/ data[sponsored == 1, .N]))`% of individual–sponsored post pairs. Second, we excluded the first and last two posts of each feed (i.e., $300 \times 4$ observations) from our analysis, as meaningful dwell times couldn't be determined for these. This is because participants were familiarizing themselves with the interface at the start and deciding whether to proceed to the next stage of the study at the end of the feeds.

#### Variables

We present summary statistics for our final sample, both for participant-level observations and self-reports as well as for user interaction data at the participant $\times$ sponsored post level (see @tbl-summary-participant-level and @tbl-summary-post-participant-level, respectively). [Merge the tables and create sub-headings or panels.]

```{r summary_tables}
summary_2  <- subset[sponsored == 1,
                   .(`Position in Feed` = displayed_sequence,
                     Likes = liked,
                     Replies = hasReply,
                     `Seconds in Viewport` = seconds_in_viewport,
                     `log(Seconds in Viewport)` = log_dwell_time,
                     `Post Height` = height,
                     `Dwell Time` = log_dwell_pixel)]

summary_1 <- subset[,
                    .(participant_label,
                      Age = age, 
                      Female = female, 
                      Desktop = is_desktop,
                      `Recall Apple` = cued_apple, 
                      `.. Bose` = cued_bose, 
                      `.. Nintendo` = cued_nintendo, 
                      `.. Samsung` = cued_samsung, 
                      `.. Whoop` = cued_whoop)] %>% unique()
```

```{r}
#| warning: false
#| label: tbl-summary-participant-level 
#| tbl-cap: Summary Statistics on the Participant-Level

# stargazer(summary_1,
#           header = FALSE, 
#           type = "html",
#           digits = 2)

# Table 1: Participant-Level Summary Statistics
stats_list <- list()

# Function to calculate summary statistics
calc_stats <- function(x) {
  c(N = length(x),
    Mean = mean(x, na.rm = TRUE),
    `St. Dev.` = sd(x, na.rm = TRUE),
    Min = min(x, na.rm = TRUE),
    Max = max(x, na.rm = TRUE))
}

# Calculate statistics for each column in summary_1
cols <- c("Age", "Female", "Desktop", "Recall Apple", ".. Bose", 
          ".. Nintendo", ".. Samsung", ".. Whoop")

participant_stats <- rbindlist(lapply(cols, function(col) {
  as.list(calc_stats(summary_1[[col]]))
}), idcol = "Statistic")

setnames(participant_stats, "Statistic", "Statistic")
participant_stats[, Statistic := cols]

# Format first table
kable(participant_stats,
      format = "html",
      caption = "Summary Statistics on the Participant-Level",
      align = c('l', 'r', 'r', 'r', 'r', 'r'),
      digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE)


```

```{r}
#| warning: false
#| label: tbl-summary-post-participant-level 
#| tbl-cap: Summary Statistics on the Sponsored Post x Participant-Level

# stargazer(summary_2,
#           header = FALSE,
#           type = "html",
#           digits = 3)

# Table 2: Sponsored Post x Participant-Level Summary Statistics
cols <- c("Position in Feed", "Likes", "Replies", "Seconds in Viewport",
          "log(Seconds in Viewport)", "Post Height", "Dwell Time")

post_stats <- rbindlist(lapply(cols, function(col) {
  as.list(calc_stats(summary_2[[col]]))
}), idcol = "Statistic")

setnames(post_stats, "Statistic", "Statistic")
post_stats[, Statistic := cols]

# Format second table
kable(post_stats,
      format = "html",
      caption = "Summary Statistics on the Sponsored Post x Participant-Level",
      align = c('l', 'r', 'r', 'r', 'r', 'r'),
      digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                full_width = FALSE)
```

##### Position in Feed

Position in Feed describes our independent variable. Because we randomized the order in which the content appeared (i.e., Position in Feed acts as a within-subject factor) and because we excluded observations positioned at the beginning and the end of the feed, we observe a sample mean of `r sprintf(fmt = "%.2f",  subset[sponsored == 1, mean(displayed_sequence)])` as well as a minimum and maximum of `r round(subset[sponsored == 1, min(displayed_sequence)])` and `r round(subset[sponsored == 1, max(displayed_sequence)])`, respectively. As, we randomly manipulated the position in which each post was displayed exogenously between subjects, each sequence in which participants browsed through ads was unique.

```{r}
#| eval: true
#| label: fig-brand-order
#| fig-cap: "Distribution of Sponsored Post Impressions by Position in Feed and Brand across All Participants"
#| fig-cap-location: top

tmp <- subset[sponsored == TRUE, 
            .(N = .N), 
            by = c("displayed_sequence", "brand")][order(brand, displayed_sequence)]
ggplot(data = tmp, 
       mapping = aes(x = displayed_sequence, y = N)) +
  facet_grid(rows = vars(brand)) +
  geom_line() +
  # geom_smooth(method = "loess", col = NA) +
  scale_y_continuous(limits = c(0, 20), expand = c(0, 0, 0.1, 0), breaks = c(0, 7.5, 15)) +
  scale_x_continuous(limits = c(1, 40), expand = c(0, 0, 0, 0), breaks = c(1, 3, 10, 20, 30, 38, 40)) +
  geom_hline(yintercept = 37.5/5, alpha = 0.5, lty = 2) +
  geom_hline(yintercept = 0, alpha = 0.75, linewidth = 1) +
  geom_vline(xintercept = 3, alpha = 0.25) +
  geom_vline(xintercept = 38, alpha = 0.25) +
  layout +
  labs(y = "Number of Sponsored Impressions", x = "Position in Feed") +
  theme(panel.grid.major.y = element_blank(),
        strip.background =element_rect(fill="#FFFFFF"),
        axis.line.x = element_blank())
```

In @fig-brand-order, each line represents the number of times a sponsored post for a specific brand appeared at each position across all participants. As the placements were fully randomized, we observed some random variability that naturally fluctuates around the expected value of 7.5 impressions per position (as indicated by dashed lines).^[This expectation is based on each of the 5 sponsored posts being shown once per participant (N=300) in a feed of 40 positions, leading to an average of $\frac{300}{40} = 7.5$ for each brand's ad placement across all positions.] Taken together, this suggests that randomization within the DICE app was effective.


##### Visibility Measures

The variable Seconds in Viewport we report in @tbl-summary-post-participant-level describes the number of seconds in which at least 50 percent of a post's pixels were visible on screen (sample mean is `r sprintf(fmt = "%.2f",  subset[sponsored == 1, mean(seconds_in_viewport)])` seconds), which we log-transform to reduce skewness. To calculate our Dwell Time measure we use throughout the analyses in this case study, we then divide it by Post Height, that is, the height in which the corresponding sponsored post was displayed on a participant's screen as this variable varies within participants and between sponsored posts. The resulting Dwell Time has a sample mean of `r sprintf(fmt = "%.3f",  subset[sponsored == 1, mean(log_dwell_pixel)])` with a Fisher-Pearson skewness of `r subset[sponsored == 1, round(digits = 2, skewness(log_dwell_pixel))]`, which is considered as approximately symmetric [@Bulmer_1979].


##### Reactions

We measured reactions such as likes and replies to individual posts. In the full sample (which includes organic posts), we observe
`r subset[has_liked_any == TRUE, length(unique(participant_label))]`
(`r subset[has_replied_any == TRUE, length(unique(participant_label))]`)
participants who liked (replied to) any post in the feed. Considering sponsored posts only, however, we only count
`r subset[has_liked_sponsored == TRUE, length(unique(participant_label))]`
(`r subset[has_replied_sponsored == TRUE, length(unique(participant_label))]`)
participants who liked (replied to) at least one post, which is why we do not consider these explicit engagement metrics in our analyses.

##### Recall

```{r}
rc_1 <- sprintf(fmt = "%.2f", 100 * summary_1[, mean(`.. Nintendo`)])
rc_2 <- sprintf(fmt = "%.2f", 100 * summary_1[, mean(`.. Whoop`)])
```


@tbl-summary-participant-level shows that recall varied by brand. Between
`r rc_1`% (Nintendo) and
`r rc_2`% (Whoop)
recalled seeing the sponsored post. Only `r subset[recalled_brand_cued == FALSE, .N, by = participant_label][N == 5, .N]` participants indicated that they did not remember seeing any sponsored post.

@tbl-show-data-1 shows an excerpt of the processed data to illustrate its nested (i.e., "long") structure.

```{r}
#| label: tbl-show-data-1
#| tbl-cap: Processed Data Analyzed in Study 1

tmp <- data[,
            .(`Participant ID` = participant_label,
              `Post ID` = doc_id,
              `Sponsored Post` = sponsored,
              Brand = brand,
              `Position in Feed` = displayed_sequence,
              Likes = liked,
              Replies = hasReply,
              `Seconds in Viewport` = seconds_in_viewport,
              `log(Seconds in Viewport)` = log_dwell_time,
              `Post Height` = height,
              `Dwell Time` = log_dwell_pixel,
              Age = age, 
              Female = female, 
              Desktop = as.logical(is_desktop),
              `Recall Apple` = cued_apple, 
              `Recall Bose` = cued_bose, 
              `Recall Nintendo` = cued_nintendo, 
              `Recall Samsung` = cued_samsung, 
              `Recall Whoop` = cued_whoop)]

kable(tmp[35:45])
```


### Analysis

To estimate the impact of ad positioning on brand recall in social media feeds, we employed a mixed-effects logistic regression model with brand fixed effects to account for the binary nature of recall outcome (recalled vs. not recalled) while considering the hierarchical structure of our data: multiple observations nested within participants and ads. The model can be expressed as:

$$
\log\left(\frac{p_{ij}}{1-p_{ij}}\right) = a + \mathbf{x}_{ij} \mathbf{b} + \sum_{k=1}^{K-1} \gamma_k Brand_k + u_i + \epsilon_{ij}
$$

where $p_{ij}$ is the probability of brand recall for the $i$-th participant and $j$-th observation, $a$ is the intercept, $\mathbf{x}_{ij}$ is a vector of continuous predictors (position in feed and dwell time) with corresponding coefficient vector $\mathbf{b}$. The term $\gamma_k$ represents the fixed effects for each brand $k$ (with one Apple serving as the reference category), and $u_i$ captures the fixed effects for each participant. Finally, $\epsilon{ij}$ represents the idiosyncratic error term for participant $i$ and observation $j$.

The random participant effects $u_i$ are justified by the experimental design, as the random assignment of ad positions ensures zero correlation between participant characteristics and the explanatory variables. Brand effects are treated as fixed parameters rather than random effects, allowing for potential correlation between brand characteristics and key explanatory variables such as dwell time.



## Case Study 2: When Does Context Harm Brands? Manipulating Feed Composition to Study Brand Safety {#sec-brand-safety-case}


```{r read_brazil_data}
short <- fread(file = "../../oFeeds/studies/brand_safety/data/processed/brand-safety-short.csv", na.strings = "")
long  <- fread(file = "../../oFeeds/studies/brand_safety/data/processed/brand-safety-long.csv", na.strings = "")
stimuli_2  <- fread(file = "../templates/docs/data/case-study-2/brazil.csv", na.strings = "")
```

```{r str-to-int}
#| echo: false
long[, displayed_sequence := as.integer(displayed_sequence)]
long[, seconds_in_viewport := as.numeric(seconds_in_viewport)]
long[, log_dwell_pixel := as.numeric(log_dwell_pixel)]
long[, log_dwell_time := as.numeric(log_dwell_time)]
```


```{r subset_brazil_data}
subset <- long[is.finite(log_dwell_pixel) & 
                     displayed_sequence < 19 & 
                     displayed_sequence > 2]
```

The primary goal of our second case study is to demonstrate another feature of the DICE app: the experimental manipulation of (advertising) context. Whereas Case Study 1 used a single set of social media posts and randomized its display sequence, this study creates different sets of content for different groups of participants while keeping one sponsored post constant across all groups.
Accordingly, we systematically manipulate the context in which participants encounter the sponsored post by varying the surrounding content they see.

We demonstrate this capability by examining brand safety in social media advertising. Brand safety refers to strategies and measures dedicated to ensuring that advertising does not appear in contexts that could harm brand reputation [@FournierSrinivasan_2023; @Porter_2021]. This concern is particularly relevant for social media advertising, where platforms use automated systems to place ads in dynamic, user-generated content environments because, these systems often lack the nuanced understanding needed to identify potentially problematic contexts. Industry reports suggest that 75% of brands have experienced brand-unsafe exposures [@GumGum_2017], with ads frequently appearing alongside problematic content despite blacklists and negative targeting strategies [@SimonovVallettiVeiga_2024; @AhmadEtAl_2024]. Not solely by _controlling_ but by _manipulating_ the sponsored post's context (i.e., organic posts), the DICE app allows us to investigate sensitive challenges in an experimental setting without risking actual brand reputation damage (which would make field settings infeasible).

### Experimental Design

To examine how brand-(un)safe contexts affect brand perceptions, we created two social media feeds that were identical in structure but varied in their content surrounding a sponsored post. Both feeds contained the same number of organic posts and one identical sponsored post, with the only difference being the thematic focus of the organic content.

To ensure ecological validity, we populated our experimental feeds with actual tweets covering Brazil: one feed featured coverage of severe flooding that claimed at least 95 lives [@Buschschlueter_2024], while the other contained typical Brazil-related content including Madonna's free concert in Rio de Janeiro, soccer matches, and travel experiences. This approach mirrors real-world scenarios where automated ad placement systems, operating primarily on keywords (e.g., "Brazil"), might place the same ad in vastly different contextual environments.

The sponsored post, a fictitious KLM advertisement promoting flights to Brazil, was identical across both conditions. The creative featured the tagline: _"Brazil's wild beauty calls! Experience nature like never before. Book your breathtaking adventure with KLM."_ This messaging, while typically appropriate for airline promotion, becomes strikingly insensitive when juxtaposed against a feed focusing on the natural disaster in the region advertised, which is precisely the type of contextual mismatch that automated systems might create.

To control for position effects identified in Case Study 1, we placed KLM's sponsored post in fifth position in both conditions. While this position remained constant, we randomized the sequence of organic posts between subjects to ensure any effects weren't driven by specific content adjacencies.


### Procedure

```{r device_type_shared_2}
#| echo: false
N <- short[, .N]
share_desktop <- round(100 * short[device_type == "Desktop", .N] / N)
share_mobile  <- round(100 * short[device_type == "Mobile", .N] / N)
share_tablet  <- round(100 * short[device_type == "Tablet", .N] / N)
```

```{r c_alpha}
#| echo: false
short[, brand_att_1 := as.numeric(as.character(brand_att_1))]
short[, brand_att_2 := as.numeric(as.character(brand_att_2))]
short[, brand_att_3 := as.numeric(as.character(brand_att_3))]

alpha <- psych::alpha(x = short[, .(brand_att_1, brand_att_2, brand_att_3)])
```

As in the first case study, participants browsed the simulated feed on their own devices
(`r share_desktop`% Desktop, `r share_mobile`% Mobile, and `r share_tablet`% Tablet). After scrolling through the feed, we redirected participants to a Qualtrics survey in which they first provided demographic information as a filler task. Subsequently, we measured whether participants recalled seeing the ads by the five brands in the feed. Specifically, we measured cued recall for which we showed participants a list of ten brands and asked them to indicate indicate which ones they remembered seeing. We also included a no-recall option to reduce false positives. 
To assess brand perceptions, participants then evaluated their attitudes toward KLM using three seven-point scales  (1 = "Negative/Unfavorable/Dislike" and 7 = "Positive/Favorable/Like"; $\alpha$ = `r round(alpha$total$raw_alpha, digits = 2)`). Subsequently, participants indicated their awareness of the Brazil flooding situation before reading a debriefing and being redirected to Prolific.

### Stimuli

Building on the CSV file structure introduced in Case Study 1, we created a file containing two distinct sets of content: nineteen organic posts for each experimental condition, plus one sponsored post that needed to appear in both feeds. To ensure the sponsored post would appear in both conditions while maintaining DICE's CSV structure, we entered the sponsored post twice in the file - once for each condition. This resulted in a file with forty rows total: nineteen organic posts for each condition plus the sponsored post appearing twice.
Each post's content was specified in columns such as `<text>` and `<username>`. We used the `<condition>` parameter to distinguish between our "appropriate" (brand-safe) and "inappropriate" (brand-unsafe) feeds, assigning each row (i.e., each post) to its respective condition.
Similar to Case Study 1, we left the `<sequence>` column empty for organic posts to enable randomization, with one key exception: the KLM sponsored post was assigned a fixed `<sequence>` value of "5" to ensure consistent positioning across conditions. We marked this post as sponsored using the `<sponsored>` parameter and included a KLM landing page URL in the `<target>` column for participants who clicked on the ad.
The resulting CSV file was uploaded to an online repository to generate a URL for the DICE app.

@tbl-stimuli-case-2 shows an excerpt of the exact CSV files we used to create the stimuli for this study. You can download that file [here](https://raw.githubusercontent.com/Howquez/DICE/refs/heads/main/studies/brand_safety/stimuli/brazil.csv){target="_blank"}.

```{r}
#| label: tbl-stimuli-case-2
#| tbl-cap: CSV File Used in Study 2

text_columns <- names(stimuli_2)[sapply(stimuli_2, is.character)]

stimuli_2[, (text_columns) := lapply(.SD, function(x) {
  x <- str_replace_all(x, "\n(?=[^.,!?;:])", " ")
  x <- str_replace_all(x, "\n(?=[.,!?;:])", "")
  x <- str_replace_all(x, "@", "")
  str_replace_all(x, "<br>", " ")
}), .SDcols = text_columns]

kable(stimuli_2[c(5, 16:23)])
```



```{r test_balance}
# Omnibus test of joint orthogonality with randomization inference

# Full model with covariates
full_model <- lm(as.numeric(as.factor(condition)) ~ female + age, data = short)

# Null model without covariates
null_model <- lm(as.numeric(as.factor(condition)) ~ 1, data = short)

anova_result <- anova(null_model, full_model)

# Observed F-statistic from ANOVA
observed_f_stat <- anova_result$F[2]

# Set up randomization inference
n_simulations <- 1000
simulated_f_stats <- numeric(length = n_simulations)

for (i in 1:n_simulations) {
  # Shuffle the treatment labels
  short[, shuffled_condition := sample(condition)]
  
  # Refit the full and null models with shuffled treatment
  shuffled_full_model <- lm(as.numeric(as.factor(shuffled_condition)) ~ female + age, data = short)
  shuffled_null_model <- lm(as.numeric(as.factor(shuffled_condition)) ~ 1, data = short)
  
  # Perform ANOVA on shuffled data
  shuffled_anova_result <- anova(shuffled_null_model, shuffled_full_model)
  
  # Store the F-statistic
  simulated_f_stats[i] <- shuffled_anova_result$F[2]
}

# Calculate the p-value based on randomization inference
balance_p <- sprintf(fmt = "%.2f", mean(simulated_f_stats >= observed_f_stat))


```

### Participants and Randomization Checks

We recruited `r short[, length(unique(participant_label))]` US-American participants ($M_{age} = `r short[, round(mean(age, na.rm = TRUE))]`$ years; `r round(short[, mean(female, na.rm = TRUE)] * 100)`% female) from Prolific. Participants were randomly assigned to view either the brand-safe feed (featuring general Brazil-related content) or the brand-unsafe feed (featuring flood coverage). A key advantage of DICE over observational and platform studies is its ability to implement true random assignment, allowing us to isolate the effect of context while canceling out other factors that might influence brand perception.
To validate DICE's randomization functionality, we examined the balance between treatment groups: as illustrated in @tbl-balance, the two treatment groups do not exhibit differences in observables. Following @KerwinRostomSterck_2024, we also found support for balanced conditions in an omnibus test of joint orthogonality with randomization inference ($p=$ `r balance_p`).


```{r}
#| warning: false
#| label: tbl-balance 
#| tbl-cap: Covariate Balance Across Conditions

balance_fem <- lm(formula = female ~ condition, data = short)
balance_age <- lm(formula = age ~ condition, data = short)

balance_table <- data.table(variables = c("Mean Age (Years)", "Female (Percent)"),
                            appropriate = c(short[condition == "appropriate", 
                                                  mean(age, na.rm = TRUE)],
                                            short[condition == "appropriate",
                                                  mean(female, na.rm = TRUE)]*100),
                            inappropriate = c(short[condition == "inappropriate", 
                                                  mean(age, na.rm = TRUE)],
                                            short[condition == "inappropriate", 
                                                  mean(female, na.rm = TRUE)]*100),
                            difference = c(summary(balance_age)$coefficients[2, 1],
                                           summary(balance_fem)$coefficients[2, 1]*100),
                            pairwise_p = c(summary(balance_age)$coefficients[2, 4],
                                           summary(balance_fem)$coefficients[2, 4]))

balance_table %>% kable(digits = 3,
                        col.names = c("Covariate",
                                      "Appropriate",
                                      "Inappropriate",
                                      "Difference",
                                      "p-value"))

```


### Data

Our dataset comprises `r short[, length(unique(participant_label))]` participants and `r format(x = subset[, .N], big.mark = ",")` observations at the participant $\times$ post level. Whereas Case Study 1 analyzed multiple sponsored posts across participants, here we focus on a single sponsored post (i.e., the KLM ad) viewed by all participants, which simplifies our analytical approach. Our final sample comprises `r format(x = short[is.finite(log_dwell_pixel), .N], big.mark = ",")` observations on the participant level, which is slightly less than the expected one observation per participant due to connectivity issues: no dwell time data were recorded for around `r sprintf(fmt = "%.2f", 100 * (short[!is.finite(log_dwell_pixel), .N] / short[, length(unique(participant_label))]))`% of the sponsored posts. 

This simplified "short" data structure has two convenient methodological implications. First, because we analyze one observation per participant rather than nested data, we can apply simpler methods such as ordinary least squares (OLS) regressions in our analyses. Second, because we only focus on one sponsored post, we do not need to divide our dwell time measure by the post's height as we did in Case Study 1. Both aspects increase the interpretability of our results.

@tbl-show-data-2 shows an excerpt of the processed data to illustrate its nested (i.e., "long") structure.

```{r}
#| label: tbl-show-data-2
#| tbl-cap: Processed Data Analyzed in Study 2

tmp <- short[,
            .(`Participant ID` = participant_label,
              `Position in Feed` = 5,
              Likes = liked,
              Replies = hasReply,
              `Seconds in Viewport` = seconds_in_viewport,
              `Dwell Time` = log_dwell_time,
              `Brand Attitudes` = brand_attitude,
              Age = age, 
              Female = female, 
              Desktop = as.logical(is_desktop),
              Recall = klm_uncued_recall)]

kable(tmp[35:45])
```